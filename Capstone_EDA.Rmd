---
title: "Capstone EDA"
author: "Jared Allen"
date: "30/09/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidytext)
library(stringr)
library(tm)
library(quanteda)
```

## Introduction
The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text. The goal of this task is to understand the basic relationships you observe in the data and prepare to build your first linguistic models.

Tasks to accomplish

Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.

Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

## Data Import

Data import has been accomplished through the use of the readLines function in R, storing the data in tibble format to adhere to tidy data principles. The three datafiles to be explored in this analysis consist of data from twitter, data from blogposts, and data from news sites.

```{r data_import, warning=FALSE, echo=TRUE}

con<- file("Data/final/en_US/en_US.twitter.txt","r")
en_us_twitter <- readLines(con)
en_us_twitter <- tibble(en_us_twitter)
close(con)

con<- file("Data/final/en_US/en_US.blogs.txt","r")
en_us_blogs <- readLines(con)
en_us_blogs <- tibble(en_us_blogs)
close(con)

con<- file("Data/final/en_US/en_US.news.txt","r")
en_us_news <- readLines(con)
en_us_news <- tibble(en_us_news)
close(con)
```

```{r data_summary}
summary(en_us_twitter)
summary(en_us_blogs)
summary(en_us_news)

```
The large amount of data (`r nrows(en_us_blogs)` rows in the blog data, `r nrows(en_us_news)` in the news data, and `r nrows(en_us_twitter)` rows in the twitter data) makes exploratory analysis if the entire dataset reseource intensive. For this reason, a randomly selected subset consisting of 10% of each of the datasets will be used to characterise the data prior to analysis.

```{r data_subset}
set.seed(061119)
en_us_blog_subset <- data.frame(data=sample(en_us_blogs$en_us_blogs,(ceiling(nrow(en_us_blogs)*0.05))),stringsAsFactors = FALSE)
en_us_news_subset <- data.frame(data=sample(en_us_news$en_us_news,(ceiling(nrow(en_us_news)*0.05))),stringsAsFactors = FALSE)
en_us_twitter_subset <- data.frame(data=sample(en_us_twitter$en_us_twitter,(ceiling(nrow(en_us_twitter)*0.05))),stringsAsFactors = FALSE)

en_us_capstone_subset <- 
  data.frame(en_us_blog_subset) %>%
  mutate(source=as.character("blog")) %>%
  bind_rows(data.frame(en_us_news_subset)) %>%
  mutate(source=ifelse(is.na(source),as.character("news"),source)) %>%
  bind_rows(data.frame(en_us_twitter_subset)) %>%
  mutate(source=ifelse(is.na(source),as.character("twitter"),source))
  
```


As the project aims to predict the next word based on prior entries, stop words will need to be retained in the final model in order to maintain grammatical order. However their high weighting necessitates their removal from this exploratory analysis, along with punctuation, numbers and symbols, which are removed using the unnest_tokens function.

Preliminray examination of the blog dataset showed that at some point, apostrophes had been converted into the symbol "â" followed by a space in the dataset. These will be removed via a str_replace prior to further analysis.

```{r data_clean}

en_us_capstone_subset$data <- str_replace(en_us_capstone_subset$data,"â ","'")

en_us_capstone_subset_clean <- #need to remove stopwords after unnest - https://www.tidytextmining.com/ngrams.html#counting-and-filtering-n-grams
  en_us_capstone_subset %>%
  filter(!data %in% stop_words$word)

```

```{r restructure_ngrams}
en_us_capstone_subset_tidy_unigrams <-
  en_us_capstone_subset_clean %>%
  unnest_tokens(unigram,data,token = "ngrams",n=1)

en_us_capstone_subset_tidy_bigrams <-
  en_us_capstone_subset_clean %>%
  unnest_tokens(bigram,data,token = "ngrams",n=2)

en_us_capstone_subset_tidy_trigrams <-
  en_us_capstone_subset_clean %>%
  unnest_tokens(trigram,data,token = "ngrams",n=3)

```

```{r counts}
en_us_capstone_subset_tidy_unigrams %>%
  count(unigram,sort=TRUE)

en_us_capstone_subset_tidy_bigrams %>%
  count(bigram,sort=TRUE)

en_us_capstone_subset_tidy_trigrams %>%
  count(trigram,sort=TRUE)

```



